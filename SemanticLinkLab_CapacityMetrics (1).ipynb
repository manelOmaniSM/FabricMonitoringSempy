{"cells":[{"cell_type":"code","source":["# Import necessary libraries from sempy_labs\n","import sempy_labs as labs\n","from sempy_labs import lakehouse as lake\n","from sempy_labs import directlake\n","import sempy_labs.report as rep\n","import sempy.fabric as fabric\n","\n","# Import standard libraries for datetime handling\n","from datetime import datetime, timezone, timedelta\n","\n","# Import pandas for DataFrame handling\n","import pandas as pd\n","\n","import json, requests\n","import time\n","\n","%load_ext sempy"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3e551fd9-a9ff-409e-adaf-245f3a2863e8"},{"cell_type":"code","source":["from sempy_labs import admin, graph\n","\n","key_vault_uri = 'https://XXXX.vault.azure.net/' # Enter your key vault URI\n","key_vault_tenant_id = 'TenantID' # Enter the key vault key to the secret storing your Tenant ID\n","key_vault_client_id = 'ClientID' # Enter the key vault key to the secret storing your Client ID (Application ID)\n","key_vault_client_secret = 'ClientSecret' # Enter the key vault key to the secret storing your Client Secret"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"96e08e32-8462-412d-bcfd-ba79ad6fe7ca","normalized_state":"finished","queued_time":"2025-04-04T09:54:34.6027493Z","session_start_time":null,"execution_start_time":"2025-04-04T09:54:34.6038009Z","execution_finish_time":"2025-04-04T09:54:34.8785851Z","parent_msg_id":"07a87af4-dd24-4d30-b570-10ee7de02dec"},"text/plain":"StatementMeta(, 96e08e32-8462-412d-bcfd-ba79ad6fe7ca, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"02adf77b-85ab-435d-b040-05877c1e3607"},{"cell_type":"markdown","source":["### Get the list of Fabric capacities and datasets"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"35836a58-22f7-486c-99fe-d6289f46081c"},{"cell_type":"code","source":["with labs.service_principal_authentication(\n","    key_vault_uri=key_vault_uri, \n","    key_vault_tenant_id=key_vault_tenant_id,\n","    key_vault_client_id=key_vault_client_id,\n","    key_vault_client_secret=key_vault_client_secret):\n","    \n","    df_capacities = labs.admin.list_capacities()\n","\n","df_capacities['run_timestamp'] = datetime.now()\n","\n","labs.save_as_delta_table(\n","                    dataframe=df_capacities,\n","                    delta_table_name=\"list_capacities\",\n","                    write_mode=\"overwrite\",\n","                    merge_schema=True\n","                )"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"db06ce37-c029-4553-8d4b-cf5b359e89c7"},{"cell_type":"code","source":["with labs.service_principal_authentication(\n","    key_vault_uri=key_vault_uri, \n","    key_vault_tenant_id=key_vault_tenant_id,\n","    key_vault_client_id=key_vault_client_id,\n","    key_vault_client_secret=key_vault_client_secret):\n","    \n","    df_datasets = labs.admin.list_datasets()\n","\n","df_datasets['run_timestamp'] = datetime.now()\n","df_datasets['Upstream Datasets'] = df_datasets['Upstream Datasets'].apply(lambda x: 'NA' if x == [] else x)\n","df_datasets['Users'] = df_datasets['Users'].apply(lambda x: 'NA' if x == [] else x)\n","\n","labs.save_as_delta_table(\n","                    dataframe=df_datasets,\n","                    delta_table_name=\"list_datasets\",\n","                    write_mode=\"overwrite\",\n","                    merge_schema=True,\n","                )\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f1ebb335-c070-414b-ba1f-948d2ce3d4c0"},{"cell_type":"markdown","source":["### Get the list Top Consumer for each capacity"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"36cc13d2-195f-4201-83c8-dbbd057a0286"},{"cell_type":"markdown","source":["##### Step 1: Run SQL query and get the list of Capacity IDs"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d3cdb90a-a089-45b8-85e6-dfce593e304c"},{"cell_type":"code","source":["capacity_df = spark.sql(\"\"\"\n","    SELECT DISTINCT Capacity_Id \n","    FROM list_capacities \n","    WHERE Capacity_Id IN ('f5b2b389-eeb1-429d-9e6e-f9ddff2c9099')\n","\"\"\")\n","capacity_id_list = capacity_df.select('Capacity_Id').rdd.flatMap(lambda x: x).collect()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"96e08e32-8462-412d-bcfd-ba79ad6fe7ca","normalized_state":"finished","queued_time":"2025-04-04T09:55:10.8577881Z","session_start_time":null,"execution_start_time":"2025-04-04T09:55:14.4247373Z","execution_finish_time":"2025-04-04T09:55:32.6035163Z","parent_msg_id":"3be14b55-365c-4a18-a5a0-68ac24c749d4"},"text/plain":"StatementMeta(, 96e08e32-8462-412d-bcfd-ba79ad6fe7ca, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":8,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5fd495a8-93f9-43c9-96c0-782cbc3bd56d"},{"cell_type":"markdown","source":["##### Step 2: Loop through Capacity IDs and run DAX queries"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dc7ea183-829b-4f8a-ae26-45994b822c5f"},{"cell_type":"code","source":["FabricMonitoringWorkspace= \"Microsoft Fabric Capacity Metrics\"\n","FabricMonitoringDataset=\"Fabric Capacity Metrics\"\n","\n","for capacity_id in capacity_id_list:\n","    dax_query = f\"\"\"\n","    DEFINE\n","        MPARAMETER 'CapacityID' = \"{capacity_id}\"\n","\n","        VAR __DS0FilterTable = \n","        TREATAS({{\"{capacity_id}\"}}, 'Capacities'[capacityId])\n","\n","    EVALUATE\n","    SUMMARIZECOLUMNS(\n","        Capacities[capacityId],\n","        Capacities[Capacity Name],\n","        Items[WorkspaceId],\n","        Items[WorkspaceName],\n","        Items[ItemKind],\n","        Items[ItemId],\n","        Items[ItemName],\n","        Dates[Date],\n","        __DS0FilterTable,\n","        \"CU\", [Dynamic M1 CU Preview],\n","        \"Duration\", [Dynamic M1 Duration Preview],\n","        \"Users\", [Dynamic M1 Users Preview],\n","        \"Memory\", round([Dynamic M1 Memory Preview],2)\n","    )\n","    \"\"\"\n","    \n","    # Run DAX queries\n","    print(f\"Running DAX for Capacity ID: {capacity_id}\")\n","    df_daxresult = fabric.evaluate_dax(FabricMonitoringDataset, dax_query, FabricMonitoringWorkspace)\n","    labs.generate_dax_query_view_url(dataset=FabricMonitoringDataset, workspace=FabricMonitoringWorkspace, dax_string=dax_query)\n","\n","\n","    # Rename columns to remove square brackets\n","    df_daxresult = df_daxresult.rename(columns={\n","        'Capacities[capacityId]': 'capacityId',\n","        'Capacities[Capacity Name]': 'capacityName',\n","        'Items[WorkspaceId]': 'workspaceId',\n","        'Items[WorkspaceName]': 'workspaceName',\n","        'Items[ItemKind]': 'itemKind',\n","        'Items[ItemId]': 'itemId',\n","        'Items[ItemName]': 'itemName',\n","        'Dates[Date]': 'date',\n","        '[CU]': 'CU',\n","        '[Duration]': 'duration',\n","        '[Users]': 'users',\n","        '[Memory]': 'memory'\n","    })\n","\n","    # Add run timestamp\n","    df_daxresult['run_timestamp'] = datetime.now()\n","    df_daxresult['run_date'] = datetime.now().date()\n","\n","    # Save Results to Delta table\n","    labs.save_as_delta_table(\n","        dataframe=df_daxresult,\n","        delta_table_name=\"capacity_metrics\",\n","        write_mode=\"append\",\n","        merge_schema=True,\n","    )\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"15f4ba85-65ab-44a1-865c-2dff7e3add85"},{"cell_type":"markdown","source":["##### Step 3: Run SQL query and get the top 20 consumer of each capacity"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b55b2ad5-a601-4841-a83b-c0846d31ae46"},{"cell_type":"code","source":["df_topconsumer = spark.sql(\"\"\"\n","WITH AggregatedMetrics AS (\n","    SELECT \n","        capacityId,\n","        capacityName,\n","        workspaceId,\n","        workspaceName,\n","        itemKind,\n","        itemId,\n","        itemName,\n","        SUM(CU) AS totalCU,             \n","        SUM(duration) AS totalDuration,   \n","        SUM(users) AS totalUsers,        \n","        MAX(memory) AS maxMemory,      \n","        run_date  \n","    FROM capacity_metrics\n","    GROUP BY \n","        capacityId,\n","        capacityName,\n","        workspaceId,\n","        workspaceName,\n","        itemKind,\n","        itemId,\n","        itemName,\n","        run_date\n","),\n","MaxRunDate AS (\n","    SELECT \n","        MAX(run_date) AS max_runDate\n","    FROM capacity_metrics\n","),\n","RankedMetrics AS (\n","    SELECT \n","        AM.capacityId,\n","        AM.capacityName,\n","        AM.workspaceId,\n","        AM.workspaceName,\n","        AM.itemKind,\n","        AM.itemId,\n","        AM.itemName,\n","        AM.totalCU,\n","        AM.totalDuration,\n","        AM.totalUsers,\n","        AM.maxMemory,\n","        AM.run_date,\n","        ROW_NUMBER() OVER (PARTITION BY AM.capacityId ORDER BY AM.totalCU DESC) AS row_num\n","    FROM AggregatedMetrics AM\n","    INNER JOIN MaxRunDate MR\n","        ON AM.run_date = MR.max_runDate\n",")\n","SELECT \n","    capacityId,\n","    capacityName,\n","    workspaceId,\n","    workspaceName,\n","    itemKind,\n","    itemId,\n","    itemName,\n","    totalCU,\n","    totalDuration,\n","    totalUsers,\n","    maxMemory,\n","    run_date\n","FROM RankedMetrics\n","WHERE row_num <= 20\n","ORDER BY capacityId, totalCU DESC;\n","\"\"\")\n","\n","# Convert to pandaframe\n","\n","df_topconsumer_pandas = df_topconsumer.toPandas()\n","df_topconsumer_pandas['run_timestamp'] = datetime.now()\n","\n","# Save result to delta table\n","\n","labs.save_as_delta_table(\n","    dataframe=df_topconsumer_pandas,\n","    delta_table_name=\"capacity_topconsumer\",\n","    write_mode=\"append\",\n","    merge_schema=True,\n",")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"d85a3b4a-af83-4bbf-a5b4-93526174b327"},{"cell_type":"markdown","source":["### Vertipaq Analyzer and BPA on Top Consumer for each capacity"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bcabef94-5bc9-4133-9b31-68c52b2e925f"},{"cell_type":"code","source":["Item_df = spark.sql(\"\"\"\n","SELECT distinct capacityId,\n","\t\t\tcapacityName,\n","\t\t\tworkspaceName,\n","\t\t\titemName as datasetName ,\n","            dataset.Configured_By as owner,\n","\t\t\tdataset.max_Created_Date as Created_Date\n","FROM capacity_topconsumer as capacity\n","INNER JOIN ( \n","\tselect Workspace_Id, Dataset_Id,Configured_By, MAX(run_timestamp) AS max_Created_Date\n","\tfrom list_datasets \n","\tGROUP BY Workspace_Id, Dataset_Id,Configured_By\n","\t) as dataset \n","\n","ON UPPER(capacity.workspaceId) = UPPER(dataset.Workspace_Id)\n","WHERE itemKind=\"Dataset\"\n","\"\"\")\n","\n","Item_df_list = Item_df.select('workspaceName','datasetName', 'owner','Created_Date').distinct().collect()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":15,"statement_ids":[15],"state":"finished","livy_statement_state":"available","session_id":"96e08e32-8462-412d-bcfd-ba79ad6fe7ca","normalized_state":"finished","queued_time":"2025-04-04T09:57:22.5255629Z","session_start_time":null,"execution_start_time":"2025-04-04T09:57:22.5268859Z","execution_finish_time":"2025-04-04T09:57:27.2331065Z","parent_msg_id":"99f06bf0-bb92-49e5-9762-680580db2ff9"},"text/plain":"StatementMeta(, 96e08e32-8462-412d-bcfd-ba79ad6fe7ca, 15, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"396aa08a-6e25-463e-a0a9-6005f4692da3"},{"cell_type":"markdown","source":["#### Run Vertipaq Analyzer"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"69d1fe88-ebd3-45fb-ae52-017a6e8bfa8a"},{"cell_type":"code","source":["def process_vertipaq(workspace_Name, dataset_Name):\n","    try:\n","        \n","        # Run Vertipaq analysis\n","        print(f\"Running Vertipaq for Workspace: {workspace_Name}, Dataset: {dataset_Name}\")\n","        labs.vertipaq_analyzer(dataset=dataset_Name, workspace=workspace_Name, export='table')\n","        result_status = \"Vertipaq scan complete.\"\n","\n","    except Exception as e:\n","        # Log failure due to an error\n","        result_status = \"Vertipaq scan NOT complete (error).\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":68,"statement_ids":[68],"state":"finished","livy_statement_state":"available","session_id":"7fdc119a-1a60-4876-af34-ec3d33222bab","normalized_state":"finished","queued_time":"2025-04-04T07:41:54.7656505Z","session_start_time":null,"execution_start_time":"2025-04-04T07:41:54.7668023Z","execution_finish_time":"2025-04-04T07:41:55.0677153Z","parent_msg_id":"8b8cf86c-a1e6-4bf2-8bf5-bc30f9abf453"},"text/plain":"StatementMeta(, 7fdc119a-1a60-4876-af34-ec3d33222bab, 68, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a302f5aa-0832-440d-a178-23973ac748c7"},{"cell_type":"code","source":["def process_vertipaq_withRefreshSkipping(workspace_Name, dataset_Name, max_creation_time):\n","    try:\n","        # Fetch dataset refresh history\n","        dataset_refresh_history_df = labs.get_semantic_model_refresh_history(dataset=dataset_Name, workspace=workspace_Name)\n","        \n","        # Ensure the 'End Time' column is parsed as datetime\n","        if 'End Time' in dataset_refresh_history_df.columns:\n","            dataset_refresh_history_df['End Time'] = pd.to_datetime(dataset_refresh_history_df['End Time'], errors='coerce')\n","\n","        # Extract the maximum End Time\n","        if not dataset_refresh_history_df.empty and dataset_refresh_history_df['End Time'].notna().any():\n","            max_end_time = dataset_refresh_history_df['End Time'].max().replace(microsecond=0, tzinfo=None)\n","        else:\n","            print(\n","                f\"Skipping: Dataset '{dataset_Name}' in Workspace '{workspace_Name}' - No Dataset refresh history available.\"\n","            )\n","            return  # Skip without making an entry \n","\n","        # Check conditions to decide processing or skipping\n","        if max_creation_time is not None:\n","            if max_end_time <= max_creation_time:\n","                print(\n","                    f\"Skipping: Dataset '{dataset_Name}' in Workspace '{workspace_Name}' - \"\n","                    f\"Not refreshed after the last Dataset Deployment. \"\n","                    f\"(Dataset Creation Time: {max_creation_time}, Last Dataset Refresh Time: {max_end_time})\"\n","                )\n","                return  # Skip without making an entry\n","        print(\n","            f\"Processing: Dataset '{dataset_Name}' in Workspace '{workspace_Name}' - \"\n","            f\"(Max Creation Time: {max_creation_time if max_creation_time else 'Null'}, Last Refresh Time: {max_end_time})\"\n","        ) \n","\n","        # Run Vertipaq analysis\n","        print(f\"Running Vertipaq for Workspace: {workspace_Name}, Dataset: {dataset_Name}\")\n","        labs.vertipaq_analyzer(dataset=dataset_Name, workspace=workspace_Name, export='table')\n","        result_status = \"Vertipaq scan complete.\"\n","\n","    except Exception as e:\n","        # Log failure due to an error\n","        result_status = \"Vertipaq scan NOT complete (error).\"\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":67,"statement_ids":[67],"state":"finished","livy_statement_state":"available","session_id":"7fdc119a-1a60-4876-af34-ec3d33222bab","normalized_state":"finished","queued_time":"2025-04-04T07:40:53.7549227Z","session_start_time":null,"execution_start_time":"2025-04-04T07:40:53.7561139Z","execution_finish_time":"2025-04-04T07:40:54.0452198Z","parent_msg_id":"c4e8f490-2b97-40eb-9e9f-d148cdbaa8cd"},"text/plain":"StatementMeta(, 7fdc119a-1a60-4876-af34-ec3d33222bab, 67, Finished, Available, Finished)"},"metadata":{}}],"execution_count":17,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2e3c06a2-4209-4764-9e59-1efeefdffbd1"},{"cell_type":"code","source":["for row in Item_df_list:\n","\n","    process_vertipaq(row.workspaceName, row.datasetName)\n","    time.sleep(5)  "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"821c7c98-c937-4448-9d34-df7d0ebe5380"},{"cell_type":"markdown","source":["##### Run BPA"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4e1591f5-dd73-47b2-bca2-9c7112e34fb8"},{"cell_type":"code","source":["for row in Item_df_list:\n","    labs.run_model_bpa(dataset=row.datasetName,\\\n","                       workspace=row.workspaceName, \\\n","                       extended=True, \\\n","                       export=True)\n","                       #Setting extended=True will fetch Vertipaq Analyzer statistics and use them to run advanced BPA rules against your model"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"79f99d2f-2097-4890-ab26-8df5ad562843"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"widgets":{},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"environment":{"environmentId":"a48d6d93-f3d0-4e76-876e-f6ac0ebfd5a9","workspaceId":"12ce80ea-024b-4163-9cc6-fe1b5168a5df"}}},"nbformat":4,"nbformat_minor":5}
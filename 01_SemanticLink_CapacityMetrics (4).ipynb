{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e551fd9-a9ff-409e-adaf-245f3a2863e8",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries from sempy_labs\n",
    "import sempy_labs as labs\n",
    "from sempy_labs import lakehouse as lake\n",
    "from sempy_labs import directlake\n",
    "import sempy_labs.report as rep\n",
    "import sempy.fabric as fabric\n",
    "\n",
    "# Import standard libraries for datetime handling\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# Import pandas for DataFrame handling\n",
    "import pandas as pd\n",
    "\n",
    "import json, requests\n",
    "import time\n",
    "\n",
    "%load_ext sempy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02adf77b-85ab-435d-b040-05877c1e3607",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-04T09:54:34.8785851Z",
       "execution_start_time": "2025-04-04T09:54:34.6038009Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "07a87af4-dd24-4d30-b570-10ee7de02dec",
       "queued_time": "2025-04-04T09:54:34.6027493Z",
       "session_id": "96e08e32-8462-412d-bcfd-ba79ad6fe7ca",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 9,
       "statement_ids": [
        9
       ]
      },
      "text/plain": [
       "StatementMeta(, 96e08e32-8462-412d-bcfd-ba79ad6fe7ca, 9, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sempy_labs import admin, graph\n",
    "\n",
    "key_vault_uri = '' # Enter your key vault URI\n",
    "key_vault_tenant_id = 'TenantID' # Enter the key vault key to the secret storing your Tenant ID\n",
    "key_vault_client_id = 'ClientID' # Enter the key vault key to the secret storing your Client ID (Application ID)\n",
    "key_vault_client_secret = 'ClientSecret' # Enter the key vault key to the secret storing your Client Secret"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35836a58-22f7-486c-99fe-d6289f46081c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Get the list of Fabric capacities and datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db06ce37-c029-4553-8d4b-cf5b359e89c7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "with labs.service_principal_authentication(\n",
    "    key_vault_uri=key_vault_uri, \n",
    "    key_vault_tenant_id=key_vault_tenant_id,\n",
    "    key_vault_client_id=key_vault_client_id,\n",
    "    key_vault_client_secret=key_vault_client_secret):\n",
    "    \n",
    "    df_capacities = labs.admin.list_capacities()\n",
    "\n",
    "df_capacities['run_timestamp'] = datetime.now()\n",
    "\n",
    "labs.save_as_delta_table(\n",
    "                    dataframe=df_capacities,\n",
    "                    delta_table_name=\"list_capacities\",\n",
    "                    write_mode=\"overwrite\",\n",
    "                    merge_schema=True\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ebb335-c070-414b-ba1f-948d2ce3d4c0",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "with labs.service_principal_authentication(\n",
    "    key_vault_uri=key_vault_uri, \n",
    "    key_vault_tenant_id=key_vault_tenant_id,\n",
    "    key_vault_client_id=key_vault_client_id,\n",
    "    key_vault_client_secret=key_vault_client_secret):\n",
    "    \n",
    "    df_datasets = labs.admin.list_datasets()\n",
    "\n",
    "df_datasets['run_timestamp'] = datetime.now()\n",
    "df_datasets['Upstream Datasets'] = df_datasets['Upstream Datasets'].apply(lambda x: 'NA' if x == [] else x)\n",
    "df_datasets['Users'] = df_datasets['Users'].apply(lambda x: 'NA' if x == [] else x)\n",
    "\n",
    "labs.save_as_delta_table(\n",
    "                    dataframe=df_datasets,\n",
    "                    delta_table_name=\"list_datasets\",\n",
    "                    write_mode=\"overwrite\",\n",
    "                    merge_schema=True,\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc13d2-195f-4201-83c8-dbbd057a0286",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Get the list Top Consumer for each capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3cdb90a-a089-45b8-85e6-dfce593e304c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Step 1: Run SQL query and get the list of Capacity IDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd495a8-93f9-43c9-96c0-782cbc3bd56d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-04T09:55:32.6035163Z",
       "execution_start_time": "2025-04-04T09:55:14.4247373Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "3be14b55-365c-4a18-a5a0-68ac24c749d4",
       "queued_time": "2025-04-04T09:55:10.8577881Z",
       "session_id": "96e08e32-8462-412d-bcfd-ba79ad6fe7ca",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, 96e08e32-8462-412d-bcfd-ba79ad6fe7ca, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "capacity_df = spark.sql(\"\"\"\n",
    "    SELECT DISTINCT Capacity_Id \n",
    "    FROM list_capacities\n",
    "\"\"\")\n",
    "capacity_id_list = capacity_df.select('Capacity_Id').rdd.flatMap(lambda x: x).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7ea183-829b-4f8a-ae26-45994b822c5f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Step 2: Loop through Capacity IDs and run DAX queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15f4ba85-65ab-44a1-865c-2dff7e3add85",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "FabricMonitoringWorkspace= \"Microsoft Fabric Capacity Metrics\"\n",
    "FabricMonitoringDataset=\"Fabric Capacity Metrics\"\n",
    "\n",
    "for capacity_id in capacity_id_list:\n",
    "    dax_query = f\"\"\"\n",
    "    DEFINE\n",
    "        MPARAMETER 'CapacityID' = \"{capacity_id}\"\n",
    "\n",
    "        VAR __DS0FilterTable = \n",
    "        TREATAS({{\"{capacity_id}\"}}, 'Capacities'[capacityId])\n",
    "\n",
    "    EVALUATE\n",
    "    SUMMARIZECOLUMNS(\n",
    "        Capacities[capacityId],\n",
    "        Capacities[Capacity Name],\n",
    "        Items[WorkspaceId],\n",
    "        Items[WorkspaceName],\n",
    "        Items[ItemKind],\n",
    "        Items[ItemId],\n",
    "        Items[ItemName],\n",
    "        Dates[Date],\n",
    "        __DS0FilterTable,\n",
    "        \"CU\", [Dynamic M1 CU Preview],\n",
    "        \"Duration\", [Dynamic M1 Duration Preview],\n",
    "        \"Users\", [Dynamic M1 Users Preview],\n",
    "        \"Memory\", round([Dynamic M1 Memory Preview],2)\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Run DAX queries\n",
    "    print(f\"Running DAX for Capacity ID: {capacity_id}\")\n",
    "    df_daxresult = fabric.evaluate_dax(FabricMonitoringDataset, dax_query, FabricMonitoringWorkspace)\n",
    "    labs.generate_dax_query_view_url(dataset=FabricMonitoringDataset, workspace=FabricMonitoringWorkspace, dax_string=dax_query)\n",
    "\n",
    "\n",
    "    # Rename columns to remove square brackets\n",
    "    df_daxresult = df_daxresult.rename(columns={\n",
    "        'Capacities[capacityId]': 'capacityId',\n",
    "        'Capacities[Capacity Name]': 'capacityName',\n",
    "        'Items[WorkspaceId]': 'workspaceId',\n",
    "        'Items[WorkspaceName]': 'workspaceName',\n",
    "        'Items[ItemKind]': 'itemKind',\n",
    "        'Items[ItemId]': 'itemId',\n",
    "        'Items[ItemName]': 'itemName',\n",
    "        'Dates[Date]': 'date',\n",
    "        '[CU]': 'CU',\n",
    "        '[Duration]': 'duration',\n",
    "        '[Users]': 'users',\n",
    "        '[Memory]': 'memory'\n",
    "    })\n",
    "\n",
    "    # Add run timestamp\n",
    "    df_daxresult['run_timestamp'] = datetime.now()\n",
    "    df_daxresult['run_date'] = datetime.now().date()\n",
    "\n",
    "    # Save Results to Delta table\n",
    "    labs.save_as_delta_table(\n",
    "        dataframe=df_daxresult,\n",
    "        delta_table_name=\"capacity_metrics\",\n",
    "        write_mode=\"append\",\n",
    "        merge_schema=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b55b2ad5-a601-4841-a83b-c0846d31ae46",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Step 3: Run SQL query and get the top 20 consumer of each capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85a3b4a-af83-4bbf-a5b4-93526174b327",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "outputs": [],
   "source": [
    "df_topconsumer = spark.sql(\"\"\"\n",
    "WITH AggregatedMetrics AS (\n",
    "    SELECT \n",
    "        capacityId,\n",
    "        capacityName,\n",
    "        workspaceId,\n",
    "        workspaceName,\n",
    "        itemKind,\n",
    "        itemId,\n",
    "        itemName,\n",
    "        SUM(CU) AS totalCU,             \n",
    "        SUM(duration) AS totalDuration,   \n",
    "        SUM(users) AS totalUsers,        \n",
    "        MAX(memory) AS maxMemory,      \n",
    "        run_date  \n",
    "    FROM capacity_metrics\n",
    "    GROUP BY \n",
    "        capacityId,\n",
    "        capacityName,\n",
    "        workspaceId,\n",
    "        workspaceName,\n",
    "        itemKind,\n",
    "        itemId,\n",
    "        itemName,\n",
    "        run_date\n",
    "),\n",
    "MaxRunDate AS (\n",
    "    SELECT \n",
    "        MAX(run_date) AS max_runDate\n",
    "    FROM capacity_metrics\n",
    "),\n",
    "RankedMetrics AS (\n",
    "    SELECT \n",
    "        AM.capacityId,\n",
    "        AM.capacityName,\n",
    "        AM.workspaceId,\n",
    "        AM.workspaceName,\n",
    "        AM.itemKind,\n",
    "        AM.itemId,\n",
    "        AM.itemName,\n",
    "        AM.totalCU,\n",
    "        AM.totalDuration,\n",
    "        AM.totalUsers,\n",
    "        AM.maxMemory,\n",
    "        AM.run_date,\n",
    "        ROW_NUMBER() OVER (PARTITION BY AM.capacityId ORDER BY AM.totalCU DESC) AS row_num\n",
    "    FROM AggregatedMetrics AM\n",
    "    INNER JOIN MaxRunDate MR\n",
    "        ON AM.run_date = MR.max_runDate\n",
    ")\n",
    "SELECT \n",
    "    capacityId,\n",
    "    capacityName,\n",
    "    workspaceId,\n",
    "    workspaceName,\n",
    "    itemKind,\n",
    "    itemId,\n",
    "    itemName,\n",
    "    totalCU,\n",
    "    totalDuration,\n",
    "    totalUsers,\n",
    "    maxMemory,\n",
    "    run_date\n",
    "FROM RankedMetrics\n",
    "WHERE row_num <= 20\n",
    "ORDER BY capacityId, totalCU DESC;\n",
    "\"\"\")\n",
    "\n",
    "# Convert to pandaframe\n",
    "\n",
    "df_topconsumer_pandas = df_topconsumer.toPandas()\n",
    "df_topconsumer_pandas['run_timestamp'] = datetime.now()\n",
    "\n",
    "# Save result to delta table\n",
    "\n",
    "labs.save_as_delta_table(\n",
    "    dataframe=df_topconsumer_pandas,\n",
    "    delta_table_name=\"capacity_topconsumer\",\n",
    "    write_mode=\"append\",\n",
    "    merge_schema=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcabef94-5bc9-4133-9b31-68c52b2e925f",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Vertipaq Analyzer and BPA on Top Consumer for each capacity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "396aa08a-6e25-463e-a0a9-6005f4692da3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-04T09:57:27.2331065Z",
       "execution_start_time": "2025-04-04T09:57:22.5268859Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "99f06bf0-bb92-49e5-9762-680580db2ff9",
       "queued_time": "2025-04-04T09:57:22.5255629Z",
       "session_id": "96e08e32-8462-412d-bcfd-ba79ad6fe7ca",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 15,
       "statement_ids": [
        15
       ]
      },
      "text/plain": [
       "StatementMeta(, 96e08e32-8462-412d-bcfd-ba79ad6fe7ca, 15, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Item_df = spark.sql(\"\"\"\n",
    "SELECT distinct capacityId,\n",
    "\t\t\tcapacityName,\n",
    "\t\t\tworkspaceName,\n",
    "\t\t\titemName as datasetName ,\n",
    "            dataset.Configured_By as owner,\n",
    "\t\t\tdataset.max_Created_Date as Created_Date\n",
    "FROM capacity_topconsumer as capacity\n",
    "INNER JOIN ( \n",
    "\tselect Workspace_Id, Dataset_Id,Configured_By, MAX(run_timestamp) AS max_Created_Date\n",
    "\tfrom list_datasets \n",
    "\tGROUP BY Workspace_Id, Dataset_Id,Configured_By\n",
    "\t) as dataset \n",
    "\n",
    "ON UPPER(capacity.workspaceId) = UPPER(dataset.Workspace_Id)\n",
    "WHERE itemKind=\"Dataset\"\n",
    "\"\"\")\n",
    "\n",
    "Item_df_list = Item_df.select('workspaceName','datasetName', 'owner','Created_Date').distinct().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d1fe88-ebd3-45fb-ae52-017a6e8bfa8a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "#### Run Vertipaq Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a302f5aa-0832-440d-a178-23973ac748c7",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-04T07:41:55.0677153Z",
       "execution_start_time": "2025-04-04T07:41:54.7668023Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "8b8cf86c-a1e6-4bf2-8bf5-bc30f9abf453",
       "queued_time": "2025-04-04T07:41:54.7656505Z",
       "session_id": "7fdc119a-1a60-4876-af34-ec3d33222bab",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 68,
       "statement_ids": [
        68
       ]
      },
      "text/plain": [
       "StatementMeta(, 7fdc119a-1a60-4876-af34-ec3d33222bab, 68, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_vertipaq(workspace_Name, dataset_Name):\n",
    "    try:\n",
    "        \n",
    "        # Run Vertipaq analysis\n",
    "        print(f\"Running Vertipaq for Workspace: {workspace_Name}, Dataset: {dataset_Name}\")\n",
    "        labs.vertipaq_analyzer(dataset=dataset_Name, workspace=workspace_Name, export='table')\n",
    "        result_status = \"Vertipaq scan complete.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log failure due to an error\n",
    "        result_status = \"Vertipaq scan NOT complete (error).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e3c06a2-4209-4764-9e59-1efeefdffbd1",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-04-04T07:40:54.0452198Z",
       "execution_start_time": "2025-04-04T07:40:53.7561139Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c4e8f490-2b97-40eb-9e9f-d148cdbaa8cd",
       "queued_time": "2025-04-04T07:40:53.7549227Z",
       "session_id": "7fdc119a-1a60-4876-af34-ec3d33222bab",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 67,
       "statement_ids": [
        67
       ]
      },
      "text/plain": [
       "StatementMeta(, 7fdc119a-1a60-4876-af34-ec3d33222bab, 67, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def process_vertipaq_withRefreshSkipping(workspace_Name, dataset_Name, max_creation_time):\n",
    "    try:\n",
    "        # Fetch dataset refresh history\n",
    "        dataset_refresh_history_df = labs.get_semantic_model_refresh_history(dataset=dataset_Name, workspace=workspace_Name)\n",
    "        \n",
    "        # Ensure the 'End Time' column is parsed as datetime\n",
    "        if 'End Time' in dataset_refresh_history_df.columns:\n",
    "            dataset_refresh_history_df['End Time'] = pd.to_datetime(dataset_refresh_history_df['End Time'], errors='coerce')\n",
    "\n",
    "        # Extract the maximum End Time\n",
    "        if not dataset_refresh_history_df.empty and dataset_refresh_history_df['End Time'].notna().any():\n",
    "            max_end_time = dataset_refresh_history_df['End Time'].max().replace(microsecond=0, tzinfo=None)\n",
    "        else:\n",
    "            print(\n",
    "                f\"Skipping: Dataset '{dataset_Name}' in Workspace '{workspace_Name}' - No Dataset refresh history available.\"\n",
    "            )\n",
    "            return  # Skip without making an entry \n",
    "\n",
    "        # Check conditions to decide processing or skipping\n",
    "        if max_creation_time is not None:\n",
    "            if max_end_time <= max_creation_time:\n",
    "                print(\n",
    "                    f\"Skipping: Dataset '{dataset_Name}' in Workspace '{workspace_Name}' - \"\n",
    "                    f\"Not refreshed after the last Dataset Deployment. \"\n",
    "                    f\"(Dataset Creation Time: {max_creation_time}, Last Dataset Refresh Time: {max_end_time})\"\n",
    "                )\n",
    "                return  # Skip without making an entry\n",
    "        print(\n",
    "            f\"Processing: Dataset '{dataset_Name}' in Workspace '{workspace_Name}' - \"\n",
    "            f\"(Max Creation Time: {max_creation_time if max_creation_time else 'Null'}, Last Refresh Time: {max_end_time})\"\n",
    "        ) \n",
    "\n",
    "        # Run Vertipaq analysis\n",
    "        print(f\"Running Vertipaq for Workspace: {workspace_Name}, Dataset: {dataset_Name}\")\n",
    "        labs.vertipaq_analyzer(dataset=dataset_Name, workspace=workspace_Name, export='table')\n",
    "        result_status = \"Vertipaq scan complete.\"\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log failure due to an error\n",
    "        result_status = \"Vertipaq scan NOT complete (error).\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821c7c98-c937-4448-9d34-df7d0ebe5380",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "for row in Item_df_list:\n",
    "\n",
    "    process_vertipaq(row.workspaceName, row.datasetName)\n",
    "    time.sleep(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1591f5-dd73-47b2-bca2-9c7112e34fb8",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "##### Run BPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f99d2f-2097-4890-ab26-8df5ad562843",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [],
   "source": [
    "for row in Item_df_list:\n",
    "    labs.run_model_bpa(dataset=row.datasetName,\\\n",
    "                       workspace=row.workspaceName, \\\n",
    "                       extended=True, \\\n",
    "                       export=True)\n",
    "                       #Setting extended=True will fetch Vertipaq Analyzer statistics and use them to run advanced BPA rules against your model"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "environment": {
    "environmentId": "a48d6d93-f3d0-4e76-876e-f6ac0ebfd5a9",
    "workspaceId": "12ce80ea-024b-4163-9cc6-fe1b5168a5df"
   },
   "lakehouse": {
    "default_lakehouse": "0878f0f2-52a3-48d3-a51f-86728227baec",
    "default_lakehouse_name": "READY4Tech",
    "default_lakehouse_workspace_id": "12ce80ea-024b-4163-9cc6-fe1b5168a5df"
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "language": "Python",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  },
  "widgets": {}
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
